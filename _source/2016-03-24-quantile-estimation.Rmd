---
layout: post
title: Quantile Estimation
excerpt: Another argument for selecting a quantile estimation type in R
language: English
author: Nanu Frechen
category: discussion
---


```{r Settings, include=FALSE, cache=F}
library(knitr)
library(viridis)
library(kfigr)
library(knitcitations)
cleanbib()
options("citation_format" = "pandoc")
bib <- read.bibtex("references.bib")
cite_options(style="html", hyperlink="to.bib")
opts_chunk$set(fig.height=7, echo=F, cache=T, dev="svg")
```

----------------------

This post is not yet finished. I post it here to get some input about how to improve it and round it up. Maybe there are some essential flaws in my thinking. Therefore I'm keen to hear from you what you think about the topic and whether my arguments are justified!

----------------------

# Neustrukturierung:

1. Motivation
    
2. The 9 different approaches to quantile estimation in R

3. Systematic bias for low sample sizes
    * shown in repeated estimations of random generated data
    * most methods show systematic bias
    * type 5 shows the lowest bias
    * Default R type (type 7) is not the best performing
    
4. Evolution of the bias with varying sample size
    * estimates applied a discretized theoretical normal distribution with varying sample size
    * Type 1 to 3 are totally erratic and shouldn't be used at all
    * all methods underestimate the spread of the outer quantiles at low sample sizes, the bias shifts outwards
    * Type 7 approaches the true value conservatively, meaning it tends to underestimate the spread of the outer quantiles ($$q_{0.125}$$ to $$q_{0.875}$$ noticeable biased for  $$n<50$$)
    * Type 6, 8 and 9 show severe overshooting in the estimation of the outer quantiles
    * Type 4 estimate quantiles systematically lower than they are

5. Evolution of the bias over the quantile range
    * shown using the the discretized theoretical normal distribution 
    * shown with random data
    
6. Which quantiles can be estimated with relatively low systematic bias?
    * define tolerable deviation
    * calculate probabilities for which the deviation starts to be greater than tolerable
7. What are the reasons behind these differences?
    * plotting positions
8. Lessons learned
    * Type 5 is the best performing (and also the first method proposed (1914) and the one that makes most sense)
    * In application the systematic bias – more severe for the outer quantiles – should be considered: Know for which $n$ which probabilities are safe to estimate quantiles for. 

----------------------

# Motivation

Compared to "true math", applied statistics is a fuzzy science: often there are no clear rules to follow – different approaches to tackle a problem exist.
Different disciplines settled on different methods, mostly to handle problems arising from low sample sizes.
Often methods are used because they where historically proposed by some of the "fathers of statistics" and no new approach was influencial enough to change common practice.

Because most statistical tools were developed in the pre-computer era, they tend to favour easy computability. Also they were prooved theoreticaly because that was the only method at that time.
Today statistical methods can be benchmarked by applying them on large amounts of pseudo-randomly generated data using the strong computation power of modern computers. This can be helpful especially when statistics deviates from the theoretical and enters the messy world of real-world, low sample size datasets.

In this article we investigate the differences between the 9 different quantile estimation methods implemented into the software package "R". We show how they perform especially for the low sample sizes of $$n<50$$ or even $$n<10$$ and how the deviations develope with increasing sample size.
We use a very visual approach, because we first of all want to create awareness about the differences.
We give some evidence about the best performing method according to our benchmarks and create some awareness about which quantiles can be calculated with tolerable bias for which sample size. 
Other than that we don't feel in the position to propose a change in pactice unless someone more rooted in the field of applied statistics picks up our approach and backs it up with more theoretical reasoning.

# The 9 different approaches to quantile estimation in R


There are many different ways to estimate the quantiles of an empirical distribution. R knows 9 different types (see help page of ```quantile()```). It is clear that for high $$n$$ the differences in these approaches become negligible. It is the low $$n$$ where they produce different estimations. All these methods where intruduced to optimize quantile estimation for sparse data sets, to get better estimations for the low $$n$$. There is a lof of mathematical argumentation and justification out there for each of these methods, but apparently non of them is convincing enough, that the whole statistical community would stick to that method. Every statistic software has different defaults and some have no alternative methods to choose from like R has.

Not accepting this state of uncertainty I wanted to find an answer that I can understand and comprehend, to the question "**which quantile estimation is best?**".

My first Idea was to find a way to visualize the differences between these nine types. 

# Systematic bias for low sample sizes

```{r systematic-bias-for-low-sample-sizes, cache=T}
par(mfrow=c(3,3), mar=c(.1,1,2,.1), oma=c(5,4,5,6))
palette("default")

repetitions <- 1000
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
n <- 10
MeanQuantile <- array(NA, dim = c(9, repetitions, length(Q)), dimnames = list(type=1:9, repetitions=1:repetitions, Q=Q))


set.seed(1)
for(type in 1:9){
  MeanQuantile[type,1,] <-  quantile(rnorm(n), Q, type=type)
  for(i in 2:repetitions){
    MeanQuantile[type,i,] <- (MeanQuantile[type,i-1,]*(i-1) + quantile(rnorm(n), Q, type=type))/i
  }
  
  #View(MeanQuantile)
  #MeanQuantile[type,repetitions,]
  #qnorm(Q)
  #MeanQuantile[type,repetitions,] - t(qnorm(Q))
}

ylim <- range(MeanQuantile[,,])

for(type in 1:9){
  plot(NULL, xlim=c(1,repetitions), ylim=ylim, main=paste("type ", type), log="", axes=F, frame.plot = T)
  if(type>6) axis(1, las=3)
  if(type%%3==1) axis(2, las=1)
  for(i in 1:length(Q)){
    #polygon(c(1,1:repetitions, repetitions), c(qnorm(Q[i]), MeanQuantile[type,,i], qnorm(Q[i])), col=c("red", "green"), border = NA)
    polygon(c(1,1:repetitions, repetitions), c(qnorm(Q[i]), pmin(qnorm(Q[i]), MeanQuantile[type,,i]), qnorm(Q[i])), col="red", border = NA)
    polygon(c(1,1:repetitions, repetitions), c(qnorm(Q[i]), pmax(qnorm(Q[i]), MeanQuantile[type,,i]), qnorm(Q[i])), col="#3fa309", border = NA)
  }
  for(i in 1:length(Q)){
    lines(MeanQuantile[type,,i], col=1)
  }
    
  if(type%%3==0) axis(4, qnorm(Q), Q, las=1)
  
}
mtext("running mean of repeated quantile estimation", 3, outer=T, cex=1, line=2.2, font=2)
mtext(paste("of random samples with n =", n, "of the standard normal distribution"), 3, outer=T, cex=0.9, line=0.5, font=1)
#mtext("number of avaraged repetitions", 1, outer=T, line=3.5)
#mtext("number of repetitions forming the running mean", 1, outer=T, line=3.5)
#mtext("number of random drawings forming the avarage", 1, outer=T, line=3.5)
mtext("number of random drawings contributing to running mean", 1, outer=T, line=3.5)
mtext("probabilities of the theoretical quantiles", 4, outer=T, line=4.5)
#mtext("quantiles estimated from random distribution", side = 2, outer = T, cex=1, line=2.5)
mtext("quantiles", side = 2, outer = T, cex=1, line=2.5)
```



For very high $$n$$ all these estimations end up where the theoretical quantiles are located (compare secondary axis on the right). The difference is how fast they approach this value for low $$n$$.

# Evolution of the bias with varying sample size 

To visualize how the bias changes with increasing sample size we went two ways. For the first we applied the different quantile estimation methods on samples directly taken from the theretical distribution function (without randomness involved). You can think of it as discretizing the distribution function by taking a finite number of values along its trajectory as shown in the next graph.

```{r theoretical-and-discrete-distribution-for-different-n, anchor="figure", dev="svg", fig.height=3.1}
set.seed(1)
par(mfrow=c(1,3), mar=c(0,1,2,.3), oma=c(5,4,4,6))
plotnum <- 1

p_i.fine <- seq(0, 1, length.out=500)
for(n in c(5, 10, 30) ){
  p_i <- seq(1/(n*2), 1-1/(n*2), length.out=n)
  if(n==30){
    q_i <- sort(rnorm(n))
  }else{
    q_i <- qnorm(p_i)
  }
  breaks <- seq(0, 1, length.out=n+1)

  plot(p_i.fine, qnorm(p_i.fine), type="l", axes=F, panel.first=
    if(n<20) {
      for(i in 1:(length(breaks)/2)*2){
        ylims <- par("usr")[3:4]
        rect(breaks[i-1], ylims[1], breaks[i], ylims[2], col = gray(.9), border = NA)
      }
      for(i in 1:length(p_i)){
        lines(rep(p_i[i], 2), c(ylims[1], q_i[i]), lty=3)
        lines(c(p_i[i], -1), rep(q_i[i],2), lty=3)
      }
      
    }
    , frame.plot=T, main=paste0("n = ", n, if(n==30) ", randomly drawn"), xlab="", ylab="", las=1, xlim=c(0,1),
    xaxs="i")

  points(p_i, q_i)
  #par(new=T)
  #hist(qnorm(p_i), ann=F, axes=F, xlim=range(qnorm(p_i.fine),finite = T))
  #axis(4, las=1)
  
  if(plotnum%%3==1){
    axis(2, las=1)
  }
  if(plotnum>0){
    axis(1)
  }
  plotnum <- plotnum + 1
}
mtext("discretizing the theoretical distribution function", cex=1, font=2, 3, outer=T, line=2.2)
mtext("in contrast to random drawings", cex=1, font=1, 3, outer=T, line=0.5)
mtext("quantiles q(p)", 2, 2.5, outer=T)
mtext("probabilities p", 1, 3.5, outer=T)
```

The data points are taken from the center of a probability range (visualized by the checkerboard background in the graph), so all data points represent exactly the same length of the total probability range.

To validate that we didn't introduce another bias by our approach we also used repeated random drawings. This was done by taking the mean of 1000 repeated random drawings from the standard normal distribution with different sample size. This is like taking the last value of our first graph and repeating the process for other sample sizes. We ended up with two very similar graphs:

```{r evolution-of-bias-with-varying-sample-size-discretized-normal-distribution, dev="svg"}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(5,4,5,6))

nmax <- 1000
nn <- 2:nmax
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
Qmatrix <- array(NA, dim=c(9, length(nn), length(Q)), dimnames=list(type=1:9, n=nn, Q=Q))

for(type in 1:9){
  
  for(n in nn){
    x <- seq(1/(n*2), 1-1/(n*2), length.out=n)
    Qmatrix[type,as.character(n),] <- quantile(qnorm(x), Q, type = type)
  }
}

ylim <- c(-1,1)*1.5

for(type in 1:9){
  #View(Qmatrix)
  
  plot(NULL, xlim=c(2,nmax), ylim=ylim, log="x", main=paste("type ", type), xlab="", ylab="", axes=F, frame.plot=T, panel.first=abline(v=c(1,2,5,10,20,50,100,200,500,1000), h=qnorm(Q), col="lightgray"))
  for(i in 1:length(Q)){
    lines(nn, Qmatrix[type,,i], col=1, type="l")
  }

  if(type%%3==0){
    axis(4, at=qnorm(Q), Q, las=1)
  }
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1, las=3)
  }
}
mtext("Evolution of quantile estimation bias with varying sample size", 3, outer=T, cex=1, line=2.2, font=2)
mtext("using a discretized standard normal distribution as input", 3, outer=T, cex=0.9, line=0.5, font=1)
mtext("probabilities corresponding to theoretical quantiles", 4, line=4.5, outer=T)  
mtext("estimated quantiles", side = 2, outer = T, cex=1, line=2.5)
mtext("sample size (n)",1, outer=T, line=3.5)

```

```{r random-experiment, cache=T}
nn <- c(2:9, signif(10^seq(1, 3, 1/3), 1))
nn2 <- c(2,3,5,10,20,50)

d <- 2^8-1
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
repetitions <- 1000

MeanQuantile <- array(NA, dim = c(9, repetitions, length(Q), length(nn)), dimnames = list(type=1:9, repetitions=1:repetitions, Q=Q, n=nn))

for(n in as.character(nn)){
  for(type in 1:9){
    set.seed(1)
    MeanQuantile[type,1,,n] <-  quantile(rnorm(n), Q, type=type)
    for(i in 2:repetitions){
      MeanQuantile[type,i,,n] <- (MeanQuantile[type,i-1,,n]*(i-1) + quantile(rnorm(n), Q, type=type))/i
    }
    
    #View(MeanQuantile)
    #MeanQuantile[type,repetitions,]
    #qnorm(Q)
    #MeanQuantile[type,repetitions,] - t(qnorm(Q))
  }
  
}

#----------------------------

```

```{r evoultion-of-bias-with-varying-sample-size-random, dependson="random-experiment"}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(5,4,6,6))
palette("default")


Qselect <- 2^5*1:7
ylim <- c(-1,1)* 1.5

for(type in 1:9){
  #View(Qmatrix)
  
  plot(NULL, xlim=c(2,nmax), ylim=ylim, log="x", main=paste("type ", type), xlab="", ylab="", axes=F, frame.plot=T, panel.first=abline(v=c(1,2,5,10,20,50,100,200,500,1000), h=qnorm(Q[Qselect]), col="lightgray"))
  for(i in Qselect){
    lines(nn, MeanQuantile[type,repetitions,i,], col=1, type="l")
  }

  if(type%%3==0){
    axis(4, at=qnorm(Q[Qselect]), Q[Qselect], las=1)
  }
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1, las=3)
  }
}
mtext("Evolution of quantile estimation bias with varying sample size", 3, outer=T, cex=1, line=3.9, font=2)
mtext("using random samples drawn from a standard normal distribution", 3, outer=T, cex=0.9, line=2.2, font=1)
mtext("(mean of 1000 estimations)", 3, outer=T, cex=0.9, line=0.5, font=1)
mtext("probabilities of the theoretical", 4, line=4.5, outer=T)  
mtext("estimated quantiles", side = 2, outer = T, cex=1, line=2.5)
mtext("sample size (n)",1, outer=T, line=3.5)
```

First of all we see that types 1 to 3 are totally erratic at low sample sizes. In the first graph we see that this jumping up an down is quite predefined by the math, since it is very regular. These methods don't look like they should be used at all.

From the remaining all underestimate the spread of the outer quantiles ("the extremes") at low sample sizes. The systematic bias gets smaller with increasing sample size. The shrinking of the bias happens later (at larger sample sizes) the more to the extremes the quantile is located.
The extreme low and upmost quantiles are apparently hardest to estimate. You need very high $$n$$ to estimate them within reasonable precision.

Type 7 approaches the true value conservatively, meaning that it never overestimates the spread of the quantiles. In the contrary it tends to underestimate the spread of the outer quantiles (everything outside of $$q_{0.125}$$ and $$q_{0.875}$$ is noticeable biased for  $$n<50$$).

Type 5, 8 and 9 show overshooting in the estimation of the outer quantiles. The overshooting phase happens later for quantiles located more at the extremes. Most severe is this overshooting for type 6. 
The overshooting is more visible in the discretized version than in the graph deduced from random drawings. This also shows in the next graphs. 

Type 4 shows a systematic underestimation of all quantiles additional to the underestimation of the spread of the outer quantiles.

Type 5 seems to have the best performance of all methods, since it comes to a fairly good estimation of the inner $$50\%$$ of quantiles for $$n=2$$ and for the inner $$75\%$$ for $$n=4$$. It also shows minimal overshooting. Not far away is the performance of types 8 and 9, especially in the graph deduced from the random drawings.



# Evolution of the bias over the quantile range 

We already established that the outer quantiles show higher deviation from the expected value and loose this systematic bias later (at higher sample sizes). The next graphs show the deviation of the estimated quantiles from the expected values over the whole probability range for some selected sample sizes.

```{r difference-theoretical-to-estimated-quantiles, dev="svg"}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(5,4,5,6))

Qmatrix <- array(NA, dim=c(9, length(nn2), length(Q)), dimnames=list(type=1:9, n=nn2, Q=Q))
for(type in 1:9){
  
  for(i in seq_along(nn2)){
    x <- seq(1/(nn2[i]*2), 1-1/(nn2[i]*2), length.out=nn2[i])
    Qmatrix[type,i,] <- quantile(qnorm(x), Q, type = type)-qnorm(Q)
  }
}

ylim <- c(-1,1)*2.5
palette(viridis(length(nn2), direction=-1, end=0.9))
# type=5
for(type in 1:9){
  plot(NULL, xlim=c(0,1), ylim=ylim, main=paste("type ", type), axes=F, frame.plot=T)
  abline(h=0, col="black")
  col=0
  for(i in seq_along(nn2)){
    col = col +1
    lines(Q, Qmatrix[type,i,], col=col, type="l")
  }
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
  if(type==6){
    legend("right", legend=as.character(nn2), col = 1:length(nn2), xpd = NA, lty=1, horiz = F, title="n =", inset = -0.45)
  }
}
mtext(paste0("deviation of the estimated quantiles from their expected values"), 3, outer=T, line=2.2, font=2, cex=1)
mtext(paste0("calculated over the whole probability range"), 3, outer=T, line=0.5, font=1, cex=1)
mtext("deviation from expected quantiles", 2, outer=T, line=3)
mtext("probabilities", 1, outer=T, line=3)

```

```{r systematic-deviation-from-theoretical-quantiles-with-varying-n, dependson="random-experiment"}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(8,5,3,6))

for(type in 1:9){
  plot(NULL, xlim=c(0,1), ylim=ylim, panel.first = abline(h=0, col="black"), axes=F, frame.plot=T, xlab="", ylab="", main=paste("type ", type))
  
  col=0
  for(n in as.character(nn2)){
    col=col + 1
 
    #points(Q, Qmatrix[type,n,]-qnorm(Q), col=n)
    #plot(qnorm(Q),Q, xlim=range(qnorm(Q))*1.3)
         #ylim=)
    lines(Q, MeanQuantile[type,repetitions,,n]-qnorm(Q), col=col)
    
  }
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
  if(type==6){
    legend("right", legend=as.character(nn2), col = 1:length(nn2), xpd = NA, lty=1, horiz = F, title="n =", inset = -0.45)
  }
}
mtext(paste0("systematic deviation from theoretical quantiles with variation of n"), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation from theoretical quantiles", 2, outer=T, line=3)
mtext("probabilities", 1, outer=T, line=3)
```

We can see that the results of methods 1 to 3 are not only erratic depending on what sample size you choose, but also depending on the quantile you choose to calculate. 

We also see the severe underestimation of most of the quantile range for type 4 together with a clear asymmetry in the deviations of the lower and the upper quantiles.

Type 5 to 9 show symmetric errors. The overshooting of methods 6, 8 and 9 for the outer quantiles is apparant, again less in the graph deduced from random drawings. 
Again type 5 seems to be the best performing with type 8 and 9 not falling far behind.

From these graphs we can learn how the deviation from the expected values is extremely more severe for the outer quantiles. But also how fast the deviation shifts to quantiles further outward with increasing sample size. This leads us to our next question about which quantiles can be safely calculated for which sample size without having to expect the large systematic deviations shown in these graphs.


# Which quantiles can be estimated with relatively low systematic bias? 

Systematic deviations of the estimated quantiles to their expected value are evident from the graphs shown so far. To minimize these systematic deviations one could define a threshold, for example $$0.1\sigma$$. We then can calculate for which quantile deviations start to go well beyond this threshold. We end up with a collection of maximum probabilities that are safe to calculate quantiles for for specific sample sizes. This is exemplary done in the next graph for type 5 and 7:

```{r deviation-greater-than-tolerance-type-5, dev="svg", fig.height=6}
par(mfrow=c(2,1), mar=c(3,1,2,.1), oma=c(2,4,3,4), cex.main=1)

tolerance=0.1
type=5
plot(NULL, xlim=c(0,1), ylim=c(-0.5,0.5), main=paste("type ", type), axes=F, frame.plot=T)
abline(h=0)
abline(h=c(-1,1)*tolerance, lty=2)
in_tolerance_low <- seq_along(nn2)
in_tolerance_up <- seq_along(nn2)
for(i in seq_along(nn2)){
  lines(Q, Qmatrix[type,i,], col=i, type="l")
  in_tolerance_low[i] <- as.numeric(names(which.max(diff(abs(Qmatrix[type,i,])<tolerance))))
  abline(v=in_tolerance_low[i], col=i)
  in_tolerance_up[i] <- as.numeric(names(which.min(diff(abs(Qmatrix[type,i,])<tolerance))))
  abline(v=in_tolerance_up[i], col=i)
}
axis(2, las=1)
axis(1, at=in_tolerance_low[1:3], labels = round(in_tolerance_low, 2)[1:3], las=2)
axis(1, at=in_tolerance_up[1:3], labels = round(in_tolerance_up, 2)[1:3], las=2)
axis(1, at=c(0,1,0.5), las=2)

legend("right", legend=as.character(nn2), col = 1:length(nn2), xpd = NA, lty=1, cex = 0.8, horiz = F, title="n =", inset = -0.15)



type=7
plot(NULL, xlim=c(0,1), ylim=c(-0.5,0.5), main=paste("type ", type), axes=F, frame.plot=T)
abline(h=0)
abline(h=c(-1,1)*tolerance, lty=2)
in_tolerance_low_7 <- seq_along(nn2)
in_tolerance_up_7 <- seq_along(nn2)
for(i in seq_along(nn2)){
  lines(Q, Qmatrix[type,i,], col=i, type="l")
  in_tolerance_low_7[i] <- as.numeric(names(which.max(diff(abs(Qmatrix[type,i,])<tolerance))))
  abline(v=in_tolerance_low_7[i], col=i)
  in_tolerance_up_7[i] <- as.numeric(names(which.min(diff(abs(Qmatrix[type,i,])<tolerance))))
  abline(v=in_tolerance_up_7[i], col=i)
}
axis(2, las=1)
axis(1, at=in_tolerance_low_7[1:5], labels = round(in_tolerance_low_7, 2)[1:5], las=2)
axis(1, at=in_tolerance_up_7[1:5], labels = round(in_tolerance_up_7, 2)[1:5], las=2)
axis(1, at=c(0,1,0.5), las=2)
axis(4, at=c(-0.1,0.1), labels=c("-0.1", " 0.1"), las=1)

mtext(paste0("deviation greater than tolerance"), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation from theoretical quantiles", 2, outer=T, line=2.5)
mtext("probabilities", 1, outer=T, line=1)
```

```{r probabilities-tolerable-to-estimate, fig.height=5, dependson="deviation-greater-than-tolerance-type-5"}
plot(nn2, in_tolerance_low, type="o", pch=20, main="probabilities safe to estimate with tolerable bias", ylab="probabilities", xlab="n", ylim=c(0,1), xlim=c(0,53), col=5, lwd=2, cex.main=1, las=1)
text(nn2, in_tolerance_low, labels = round(in_tolerance_low, 2), adj = c(0,-.6), cex=0.9, col=5)
lines(nn2, in_tolerance_up, type="o", pch=20, col=5, lwd=2)
text(nn2, in_tolerance_up, labels = round(in_tolerance_up, 2), adj = c(0,1.8), cex=0.9, col=5)

lines(nn2, in_tolerance_low_7, type="o", pch=20, col=2, lwd=2)
text(nn2, in_tolerance_low_7, labels = round(in_tolerance_low_7, 2), adj = c(0,-.6), cex=0.9, col=2)
lines(nn2, in_tolerance_up_7, type="o", pch=20, col=2, lwd=2)
text(nn2, in_tolerance_up_7, labels = round(in_tolerance_up_7, 2), adj = c(0,1.6), cex=0.9, col=2)

legend("right", legend=c(5,7), col = c(5,2), xpd = NA, lty=1, horiz = F, title="type =", inset = 0.05, bty="n", lwd=2)
```

We can see that for method type 5 the outer quantiles become calculable with bias less than $$0.1\sigma$$ for far lower sample sizes than is the case for type 7. 

-----------------------------------
 
```{r comparison-between-discrete-and-random-repetition, dev="svg", eval=F}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(8,5,3,6))

ylim <- max(abs(range(c(t(MeanQuantile[,repetitions,])-qnorm(Q), t(Qmatrix[,n,])-qnorm(Q))))) * c(-1,1)

for(type in 1:9){
  #plot(qnorm(Q),Q, xlim=range(qnorm(Q))*1.3)
  plot(Q, Qmatrix[type,n,]-qnorm(Q), xlim=c(0,1), ylim=ylim, panel.first = abline(h=0, col="lightgray"), axes=F, frame.plot=T, xlab="", ylab="", main=paste("type ", type))
       #ylim=)
  points(Q, MeanQuantile[type,repetitions,]-qnorm(Q), col="red")
  
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
}
mtext(paste0("systematic deviation between estimated and theoretical quantiles"), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation from theoretical quantiles", 2, outer=T, line=3)
mtext("Quantiles estimated from a distribution of n=10. \nQuantiles estimated from discrete distribution in black, from repeated random generation in red.", 1, outer = T, line=6, cex=0.9)
mtext("probabilities", 1, outer=T, line=3)

```


```{r comparison-between-discrete-and-random-repetition-with-n-20, eval=F, dev="svg"}

par(mfrow=c(3,3), mar=c(.1,1,2,.1), oma=c(5,4,3,6))

repetitions <- 1000
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
n <- 20
MeanQuantile <- array(NA, dim = c(9, repetitions, length(Q)), dimnames = list(type=1:9, repetitions=1:repetitions, Q=Q))


for(type in 1:9){
  set.seed(1)
  MeanQuantile[type,1,] <-  quantile(rnorm(n), Q, type=type)
  for(i in 2:repetitions){
    MeanQuantile[type,i,] <- (MeanQuantile[type,i-1,]*(i-1) + quantile(rnorm(n), Q, type=type))/i
  }
  
  #View(MeanQuantile)
  #MeanQuantile[type,repetitions,]
  #qnorm(Q)
  #MeanQuantile[type,repetitions,] - t(qnorm(Q))
}

#----------------------------

par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(8,5,3,6))

ylim <- max(abs(range(c(t(MeanQuantile[,repetitions,])-qnorm(Q), t(Qmatrix[,n,])-qnorm(Q))))) * c(-1,1)

for(type in 1:9){
  #plot(qnorm(Q),Q, xlim=range(qnorm(Q))*1.3)
  plot(Q, Qmatrix[type,n,]-qnorm(Q), xlim=c(0,1), ylim=ylim, panel.first = abline(h=0, col="lightgray"), axes=F, frame.plot=T, xlab="", ylab="", main=paste("type ", type))
       #ylim=)
  points(Q, MeanQuantile[type,repetitions,]-qnorm(Q), col="red")
  
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
}
mtext(paste0("systematic deviation from theoretical quantiles with n = ",n), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation of estimated quantile values from theoretical", 2, outer=T, line=3)
mtext("quantiles estimated from discrete distribution in black, from repeated random generation in red", 1, outer = T, line=6, cex=0.9)
mtext("quantiles", 1, outer=T, line=3)

```


```{r comparison-between-discrete-and-random-repetition-with-n-21, eval=FALSE}
par(mfrow=c(3,3), mar=c(.1,1,2,.1), oma=c(5,4,3,6))

repetitions <- 1000
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
n <- 21
MeanQuantile <- array(NA, dim = c(9, repetitions, length(Q)), dimnames = list(type=1:9, repetitions=1:repetitions, Q=Q))


for(type in 1:9){
  set.seed(1)
  MeanQuantile[type,1,] <-  quantile(rnorm(n), Q, type=type)
  for(i in 2:repetitions){
    MeanQuantile[type,i,] <- (MeanQuantile[type,i-1,]*(i-1) + quantile(rnorm(n), Q, type=type))/i
  }
  
  #View(MeanQuantile)
  #MeanQuantile[type,repetitions,]
  #qnorm(Q)
  #MeanQuantile[type,repetitions,] - t(qnorm(Q))
}


par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(8,5,3,6))

ylim <- max(abs(range(c(t(MeanQuantile[,repetitions,])-qnorm(Q), t(Qmatrix[,n,])-qnorm(Q))))) * c(-1,1)

for(type in 1:9){
  #plot(qnorm(Q),Q, xlim=range(qnorm(Q))*1.3)
  plot(Q, Qmatrix[type,n,]-qnorm(Q), xlim=c(0,1), ylim=ylim, panel.first = abline(h=0, col="lightgray"), axes=F, frame.plot=T, xlab="", ylab="", main=paste("type ", type))
       #ylim=)
  points(Q, MeanQuantile[type,repetitions,]-qnorm(Q), col="red")
  
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
}
mtext(paste0("systematic deviation from theoretical quantiles with n = ",n), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation of estimated quantile values from theoretical", 2, outer=T, line=3)
mtext("quantiles estimated from discrete distribution in black, from repeated random generation in red", 1, outer = T, line=6, cex=0.9)
mtext("quantiles", 1, outer=T, line=3)

```





```{r using-random-generator-with-n-100, eval=F}

par(mfrow=c(3,3), mar=c(.1,1,2,.1), oma=c(5,4,3,6))

repetitions <- 1000
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
n <- 100
MeanQuantile <- array(NA, dim = c(9, repetitions, length(Q)), dimnames = list(type=1:9, repetitions=1:repetitions, Q=Q))


for(type in 1:9){
  set.seed(1)
  MeanQuantile[type,1,] <-  quantile(rnorm(n), Q, type=type)
  for(i in 2:repetitions){
    MeanQuantile[type,i,] <- (MeanQuantile[type,i-1,]*(i-1) + quantile(rnorm(n), Q, type=type))/i
  }
  
  #View(MeanQuantile)
  #MeanQuantile[type,repetitions,]
  #qnorm(Q)
  #MeanQuantile[type,repetitions,] - t(qnorm(Q))
}

ylim <- range(MeanQuantile[,,])

for(type in 1:9){
  plot(NULL, xlim=c(1,repetitions), ylim=ylim, panel.first=abline(h=qnorm(Q), v=c(1,5,10,50,100,500,1000), col="lightgray"), main=paste("type ", type), log="x", axes=F, frame.plot = T)
  if(type>6) axis(1, las=3)
  if(type%%3==1) axis(2, las=1)
  for(i in 1:length(Q)){
    lines(MeanQuantile[type,,i], col=1)
  }
  if(type%%3==0) axis(4, qnorm(Q), Q, las=1)
  
}
mtext(paste("mean of repeated quantile estimation with n =", n), 3, outer=T, line=1, font=2, cex=1)
mtext("number of repetitions", 1, outer=T, line=3.5)
mtext("theoretical probabilities corresponding to the quantiles", 4, outer=T, line=4.5)
mtext("mean estimated value for standard distribution", side = 2, outer = T, cex=1, line=2.5)
```

```{r comparison-between-discrete-and-random-repetition-with-n-100, eval=F}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(8,5,3,6))

ylim <- max(abs(range(c(t(MeanQuantile[,repetitions,])-qnorm(Q), t(Qmatrix[,10,])-qnorm(Q))))) * c(-1,1)

for(type in 1:9){
  #plot(qnorm(Q),Q, xlim=range(qnorm(Q))*1.3)
  plot(Q, Qmatrix[type,n,]-qnorm(Q), xlim=c(0,1), ylim=ylim, panel.first = abline(h=0, col="lightgray"), axes=F, frame.plot=T, xlab="", ylab="", main=paste("type ", type))
       #ylim=)
  points(Q, MeanQuantile[type,repetitions,]-qnorm(Q), col="red")
  
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
}
mtext(paste0("systematic deviation from theoretical quantiles with n = ",n), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation of estimated quantile values from theoretical", 2, outer=T, line=3)
mtext("quantiles estimated from discrete distribution in black, from repeated random generation in red", 1, outer = T, line=6, cex=0.9)
mtext("quantiles", 1, outer=T, line=3)

```

```{r distribution-estimation-with-different-methods-lognormal, eval=F}
par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(5,4,3,6))

nmax <- 1000
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
Qmatrix <- array(NA, dim=c(9, nmax, length(Q)), dimnames=list(type=1:9, n=2:nmax, Q=Q))

for(type in 1:9){
  
  for(n in 2:nmax){
    x <- seq(1/(n*2), 1-1/(n*2), length.out=n)
    Qmatrix[type,n,] <- quantile(qlnorm(x), Q, type = type)
  }
}

ylim <- range(Qmatrix[,,])

for(type in 1:9){
  #View(Qmatrix)
  
  plot(NULL, xlim=c(1,nmax), ylim=ylim, log="x", main=paste("type ", type), xlab="", ylab="", axes=F, frame.plot=T, panel.first=abline(v=c(1,2,5,10,20,50,100,200,500,1000), h=qlnorm(Q), col="lightgray"))
  for(i in 1:length(Q)){
    lines(Qmatrix[type,,i], col=1, type="l")
  }

  if(type%%3==0){
    axis(4, at=qlnorm(Q), Q, las=1)
  }
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1, las=3)
  }
}
mtext("quantile estimation from a discrete lognormal distribution", 3, outer=T, cex=1.2, line=1, font=2)
mtext("quantiles of theoretical distribution", 4, line=4.5, outer=T)  
mtext("estimated quantile value on standard distribution", side = 2, outer = T, cex=1, line=2.5)
mtext("number of points uniformly distributed over quantiles",1, outer=T, line=3.5)

```

```{r comparison-between-discrete-and-random-repetition-lognormal-with-n-10, eval=F}

par(mfrow=c(3,3), mar=c(.1,1,2,.1), oma=c(5,4,3,6))

repetitions <- 1000
d <- 7
Q <- seq(1/(d+1), 1-1/(d+1), length.out=d)
n <- 10
MeanQuantile <- array(NA, dim = c(9, repetitions, length(Q)), dimnames = list(type=1:9, repetitions=1:repetitions, Q=Q))


for(type in 1:9){
  set.seed(1)
  MeanQuantile[type,1,] <-  quantile(rlnorm(n), Q, type=type)
  for(i in 2:repetitions){
    MeanQuantile[type,i,] <- (MeanQuantile[type,i-1,]*(i-1) + quantile(rlnorm(n), Q, type=type))/i
  }
  
  #View(MeanQuantile)
  #MeanQuantile[type,repetitions,]
  #qlnorm(Q)
  #MeanQuantile[type,repetitions,] - t(qlnorm(Q))
}

#----------------------------

par(mfrow=c(3,3), mar=c(0,1,2,.1), oma=c(8,5,3,6))

ylim <- max(abs(range(c(t(MeanQuantile[,repetitions,])-qlnorm(Q), t(Qmatrix[,10,])-qlnorm(Q))))) * c(-1,1)

for(type in 1:9){
  #plot(qlnorm(Q),Q, xlim=range(qlnorm(Q))*1.3)
  plot(Q, Qmatrix[type,n,]-qlnorm(Q), xlim=c(0,1), ylim=ylim, panel.first = abline(h=0, col="lightgray"), axes=F, frame.plot=T, xlab="", ylab="", main=paste("type ", type))
       #ylim=)
  points(Q, MeanQuantile[type,repetitions,]-qlnorm(Q), col="red")
  
  if(type%%3==1){
    axis(2, las=1)
  }
  if(type>6){
    axis(1)
  }
}
mtext(paste0("systematic deviation from theoretical lognormal quantiles with n = ",n), 3, outer=T, line=1, font=2, cex=1)
mtext("deviation of estimated quantile values from theoretical", 2, outer=T, line=3)
mtext("quantiles estimated from discrete distribution in black, from repeated random generation in red", 1, outer = T, line=6, cex=0.9)
mtext("quantiles", 1, outer=T, line=3)

```

Finally there is yet another method that sort of calculates quantiles. The box of a boxplot marks the 0.25, 0.5 and 0.75 quantiles. But the calculation is yet again different to all the other methods. Using again our above established method of visualizing the calculated quantiles with a discrete normal distribution, we can see that the upper and lower hinge jump up and down with every data point we add to the distribution. From the help page of `boxplot.stats` we can read that "The hinges equal the quartiles for odd n" and differ for even n. The wiggling is neglectable above around $$n=20$$. The upper and lower whisker spread more and more until they reach their set maximum of 1.5 times the inter quantile range (IQR) from the hinges.  After the whiskers first reach their maximum, they jump down and up again several times.

```{r boxplot_quantiles, fig.width=7, fig.height=6}
par(mar=c(4,4,5,6.5))


Q <- c(0.25,0.5,0.75)
nmax <- 1000
Qmatrix <- data.frame(lower_whisker=NA, lower_hinge=NA, median=NA, upper_hinge=NA, upper_whisker=NA)

for(n in 1:nmax){
    x <- seq(1/(n*2), 1-1/(n*2), length.out=n)
    Qmatrix[n,] <- boxplot(qnorm(x), plot = F)$stats
}

ylim <- max(abs(Qmatrix))*c(-1.3,1.3)
cols <- c(1,2,3,2,1)

plot(NULL, xlim=c(1,nmax), ylim=ylim, log="x", lwd=2,
     main='"quantiles" calculated to draw boxplots', xlab="", ylab="", frame.plot=T, panel.first=abline(v=c(1,2,5,10,20,50,100,200,500,1000), h=qnorm(Q), col="lightgray"), las=1)
for(i in 1:5){
  lines(Qmatrix[,i], col=cols[i], type="l", lwd=2)
}
axis(4, at=qnorm(Q), Q, las=1)
abline(h=c(-1,1)*(2*1.5+1)*qnorm(0.75), col="gray")
axis(4, at=c(-1,1)*(2*1.5+1)*qnorm(0.75), c("1.5 x IQR\nfrom lower\nhinge","1.5 x IQR\nfrom upper\nhinge"), las=1)
legend("topleft", legend = rev(colnames(Qmatrix)), col=cols, lty=1, bg="white")
mtext("probabilities corresponding to theoretical quantiles", 4, line=5.5, outer=F)  
mtext("estimated quantiles", side = 2, outer = F, cex=1, line=2.5)
mtext("number of points uniformly distributed over probabilities",1, outer=F, line=2.5)



# plot example boxplot
bxp(boxplot(qnorm(x), plot = F), medcol=3, staplecol=1, boxcol=2, whisklty=1, lwd=2, ylim=ylim, axes=F, add=T, at=1000, pch=20, cex=0.5)


```


# Conclusion

... still to be written ...


----------------------------

# Appendix

## R's 9 different quantile estimation functions

[Since 2004](https://stat.ethz.ch/pipermail/r-announce/2004/000427.html) R has implemented 9 types of quantile estimation recommended by `r citet("Hyndman and Fan (1996)")`:

> $$Q_i(p)= (1-\gamma) x_j ++ \gamma x_{j+1}$$
>
> where $$1 ≤ i ≤ 9$$, $$(j-m)/n ≤ p < (j-m+1)/n$$, $$x_j$$ is the $$j$$th order statistic, $$n$$ is the sample size, the value of $$\gamma$$ is a function of $$j = floor(np + m)$$ and $$g = np + m - j$$, and $$m$$ is a constant determined by the sample quantile type.
>
> ### Discontinuous sample quantile types 1, 2, and 3
>
> For types 1, 2 and 3, $$Q_i(p)$$ is a discontinuous function of $$p$$, with $$m = 0$$ when $$i = 1$$ and $$i = 2$$, and $$m = -1/2$$ when $$i = 3$$.
>
> Type&nbsp;1: | Inverse of empirical distribution function. $$\gamma = 0$$ if $$g = 0$$, and $$1$$ otherwise.
> Type&nbsp;2: | Similar to type 1 but with averaging at discontinuities. $$\gamma = 0.5$$ if $$g = 0$$, and $$1$$ otherwise.
> Type&nbsp;3: | SAS definition: nearest even order statistic. $$\gamma = 0$$ if $$g = 0$$ and $$j$$ is even, and $$1$$ otherwise.
>
> ### Continuous sample quantile types 4 through 9
> 
> For types 4 through 9, $$Q_i(p)$$ is a continuous function of $$p$$, with $$\gamma = g$$ and $$m$$ given below. The sample quantiles can be obtained equivalently by linear interpolation between the points $$(p_k,x_k)$$ where $$x_k$$ is the $$k$$th order statistic. Specific expressions for $$p_k$$ are given below.
>
> Type&nbsp;4: | `r #citet("Parzen (1979) Nonparametric statistical data modeling")` | $$m = 0$$ | $$p_k = \frac{k}{n}$$ | That is, linear interpolation of the empirical cdf.
> Type&nbsp;5: | `r citet(bib["hazen1914storage"])` | $$m = \frac{1}{2}$$ | $$p_k = \frac{k - 0.5}{n}$$ | That is a piecewise linear function where the knots are the values midway through the steps of the empirical cdf. This is popular amongst hydrologists.
> Type&nbsp;6: | `r citet(bib["weibull1939phenomenon"])`, `r citet(bib["gumbel1939probabilite"])` | $$m = p$$ | $$p_k = \frac{k}{n + 1}$$ | Thus $$p_k = E[F(x[k])]$$. This is used by Minitab and by SPSS.
> Type&nbsp;7: | `r citet(bib["gumbel1939probabilite"])` | $$m = 1-p$$ |  $$p_k = \frac{k - 1}{n - 1}$$ | In this case, $$p_k = mode[F(x[k])]$$. This is used by S and by R < 2.0.0.
> Type&nbsp;8: | `r citet(bib["johnson1970"])` |$$m = \frac{p+1}{3}$$ | $$p_k = \frac{k - 1/3}{n + 1/3}$$ | Then $$p_k \approx median[F(x[k])]$$. The resulting quantile estimates are approximately median-unbiased regardless of the distribution of $$x$$.
> Type&nbsp;9: | `r citet(bib["blom1958"])` | $$m = \frac{p}{4} + \frac{3}{8}$$ | $$p_k = \frac{k - 3/8}{n + 1/4}$$ | The resulting quantile estimates are approximately unbiased for the expected order statistics if $$x$$ is normally distributed.
>
> Further details are provided in `r citet("Hyndman_1996")` who recommended type 8. The default method is type 7, as used by S and by R < 2.0.0.

```{r plotting-positions, anchor="figure", fig.cap="Visualisation of plotting positions $p_k$ for a dataset of 10 values, as recommended by different authors", fig.height=4.5, dev="tikz", echo=F, fig.width=8, fig.show="hide"}

par(mar=c(3,3,3,15.5), pch=3, mgp=c(2,.5,0), cex=1.4, lwd=2)

plot(0, type="n", axes=F, xlim=c(0,1), ylim=c(-7.2,0.2), xlab="plotting positions $p_k$ for $k=1,...,10$", ylab="", main="$p_k=\\frac{k-a}{n+1-2a}$", cex.main=1, font.main=1)
axis(1, 0:10/10)
abline(v=0:10/10, lty=1, col="lightgray")
axis(4, 0:-7, c(
  'Parzen (1979), type 4', 
  "$$a=0$$, Weibull (1939), type 6", 
  "$a=\\frac{1}{3}$, Johnson and Kotz (1970), type 8",
  "$a=0.375$, Blom (1958), type 9", 
  "$a=0.4$, Cunnane (1987)", 
  "$a=0.44$, Gringorten (1963)",
  "$a=0.5$, Hazen (1914), type 5",
  "$a=1$, Gumbel (1939), type 7, R Default"
  ), las=1, tick=F)
axis(2, 0:-7, c(
  "$\\frac{k}{n}$", 
  "$\\frac{k}{n+1}$", 
  "$\\frac{k-1/3}{n+1/3}$",
  "$\\frac{k-0.375}{n+0.25}$", 
  "$\\frac{k-0.4}{n+0.2}$", 
  "$\\frac{k-0.44}{n+0.12}$",
  "$\\frac{k-0.5}{n}$",
  "$\\frac{k-1}{n-1}$"
  ), las=1, tick=F)


n=10
i = 1:n

p <- function(i, n, a){
  return( (i-a)/(n+1-2*a))
}

greater <- function(p1, ypos){
  p2 <- p(i,n,0.5)
  rect(pmin(p1, p2), rep(0.1-ypos, n), pmax(p1, p2), rep(-0.1-ypos,n), col=c("#3fa309", "red")[(p1 < p2)+1], border=NA)
  
}


greater( i/n, 0)
points( i/n, rep(0, n)) # Parzen
greater(p(i,n,0), 1)
points( p(i,n, 0), rep(-1,n)) # Weibull
greater(p(i,n, 1/3), 2)
points( p(i,n, 1/3), rep(-2,n)) # Johnson and Kotz (1970)

greater(p(i,n, 0.375), 3)
points( p(i,n, 0.375), rep(-3,n)) # Blom

greater(p(i,n, 0.4), 4)
points( p(i,n, 0.4), rep(-4,n)) # Cunnane

greater(p(i,n, 0.44), 5)
points( p(i,n, 0.44), rep(-5,n)) # Gringorten

points( p(i,n, 0.5), rep(-6,n)) # Hazen

greater(p(i,n, 1), 7)
points( p(i,n, 1), rep(-7,n)) # Gumbel S/R default
abline(h=0:-7)

```



```{r potting-positions-figure, results="hide", cache=FALSE}
system("pdf2svg ../figure/source/2016-03-24-quantile-estimation/plotting-positions-1.pdf ../figure/source/2016-03-24-quantile-estimation/plotting-positions-1.svg")
```

![svg-graphic](../figure/source/2016-03-24-quantile-estimation/plotting-positions-1.svg)

## The discretized standard normal distribution


So I thought about, how I could create input data for the ```quantile()``` function, that follows perferctly a theoretical distribution function---but is also discrete, featuring a limited $$n$$. 

What I came up with is illustrated in `r figr("theoretical-and-discrete-distribution-for-different-n", TRUE, TRUE, type="figure")`. I devided the probability range into equal lengths. For $$n=3$$ for example I devided it into three sections, for $$n=10$$ into 10 sections (visualized in the graph with alternating gray and white areas). Each section will be represented by a probability located in the middle of the section. This is represented in the graph with the horizontal dotted lines. Where the horizontal lines cut the theoretical distribution funtion (in this case a normal distribution) defines the value of the quantile corresponding to this probability. The quantiles can be read from the x-axis. Notice how the horizontal lines (the probabilities) are equidistant. The vertical lines (the quantiles) follow the distribution function which causes the outermost quantiles beeing further apart than the central ones.



This method generates discrete values that behave like a perfect representation of the theoretical distribution function. In contrast to using a random generator, which gives us values that jump around the theoretical distribution function  (like is illustrated in the $$n=30$$ plot above) and approach the theoretical distribution only for very high $$n$$.


Mathematically this process is done like this: You define your $$n$$. For example $$n=10$$. Now you calculate the probabilities:

$$p_k = \frac{1}{2n} + \frac{k-1}{n}$$

The last value is 

$$p_n = \frac{1}{2n} + \frac{n-1}{n}= 1-\frac{1}{2n}$$

In R this can be implemented like this:
```{r, echo=T}
n <- 10
k <- 1:n
(p_k <- 1/(2*n) + (k-1)/n)
```

Or like this:

```{r, echo=T}
(p_k <- seq(1/(n*2), 1-1/(n*2), length.out=n))
```

We then have to convert the probabilities $$p_k$$ to the quantiles. For example with a normal distribution:
```{r echo=T}
qnorm(p_k)
```


With this method I was able to produce the following graph. 
It shows the result of the `quantile()` function executed with different `type` argument. Estimated are the quantiles  corresponding to probabilities $$p_k = 0.125, 0.25, 0.375, 0.5, 0.625, 0.75$$ and $$0.875$$. You can see that the results differ for different $$n$$ (varied on the x-axis).


--------------------

# References

```{r references, results='asis', echo=FALSE, cache=F}
bibliography()
```


